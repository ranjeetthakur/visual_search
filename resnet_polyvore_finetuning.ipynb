{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvJCmmvXQ_Xv",
        "outputId": "4d03f595-5a04-4e94-cdf3-6d99068fde66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/480.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow datasets transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dnepozitek/polyvore-outfits\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_rsZP7Y5aIX",
        "outputId": "5313be58-be99-4d6b-e44b-06d548e8f527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dnepozitek/polyvore-outfits?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.21G/6.21G [04:39<00:00, 23.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/dnepozitek/polyvore-outfits/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "polyvore_dir = 'polyvore_outfits'\n",
        "filename = os.path.join(polyvore_dir, 'polyvore_item_metadata.json')"
      ],
      "metadata": {
        "id": "21h2wLUy79Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename,'r') as fl:\n",
        "    data = json.load(fl)\n",
        "\n",
        "df = pd.DataFrame(data).T\n",
        "\n",
        "df['id'] = df.index\n",
        "df['path'] = df['id'].apply(lambda x: os.path.join(polyvore_dir, 'images', x + '.jpg'))\n",
        "df['label'] = df['category_id']\n",
        "\n",
        "df.head()\n",
        "df[['id', 'path', 'label']].to_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata.csv'),index=False)"
      ],
      "metadata": {
        "id": "ylZ-2N-z8AEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def split_data_with_limit(df, k=100, train_ratio=0.8, seed=42):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets with a limit of k images per category.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a 'category_id' column.\n",
        "        k (int): Maximum number of images per category in both train and test sets.\n",
        "        train_ratio (float): Proportion of data to allocate to the train set.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame, pd.DataFrame: Train and test DataFrames.\n",
        "    \"\"\"\n",
        "    # Group data by category_id\n",
        "    grouped = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        grouped[row['label']].append(row)\n",
        "\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "    random.seed(seed)  # Set random seed for reproducibility\n",
        "\n",
        "    for category, items in grouped.items():\n",
        "        # Shuffle the items within each category\n",
        "        random.shuffle(items)\n",
        "\n",
        "        # Limit to k images per category\n",
        "        items = items[:k]\n",
        "\n",
        "        # Split into train and test sets\n",
        "        split_idx = int(len(items) * train_ratio)\n",
        "        train_data.extend(items[:split_idx])\n",
        "        test_data.extend(items[split_idx:])\n",
        "\n",
        "    # Convert back to DataFrame\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "    test_df = pd.DataFrame(test_data)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Split the data with a limit of 100 images per category\n",
        "train_df, test_df = split_data_with_limit(df, k=100, train_ratio=0.8)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# Verify category distributions\n",
        "print(\"Train set distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"\\nTest set distribution:\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "# Save to CSV\n",
        "train_df.to_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata_train.csv'),index=False)\n",
        "test_df.to_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata_test.csv'),index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDJ4yyK8Bx2",
        "outputId": "954a7ec6-3223-4e11-ae7a-a98aeee229b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 8845\n",
            "Test set size: 2239\n",
            "Train set distribution:\n",
            "label\n",
            "15      80\n",
            "4496    80\n",
            "51      80\n",
            "10      80\n",
            "310     80\n",
            "        ..\n",
            "306      4\n",
            "269      4\n",
            "289      4\n",
            "4468     3\n",
            "279      1\n",
            "Name: count, Length: 153, dtype: int64\n",
            "\n",
            "Test set distribution:\n",
            "label\n",
            "15      20\n",
            "28      20\n",
            "231     20\n",
            "51      20\n",
            "10      20\n",
            "        ..\n",
            "4473     1\n",
            "279      1\n",
            "306      1\n",
            "289      1\n",
            "4468     1\n",
            "Name: count, Length: 153, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "df = pd.read_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata.csv'))\n",
        "train_df = pd.read_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata_train.csv'))\n",
        "test_df = pd.read_csv(os.path.join(polyvore_dir, 'polyvore_item_metadata_test.csv'))"
      ],
      "metadata": {
        "id": "ho-vnaBD8Sje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(df, image_dir, target_size=(224, 224), batch_size=128):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,  # Normalize pixel values\n",
        "        # horizontal_flip=True,  # Data augmentation\n",
        "        validation_split=0.01  # Split training into train/validation\n",
        "    )\n",
        "\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=image_dir,\n",
        "        x_col=\"path\",  # Column with image filenames\n",
        "        y_col=\"label\",  # Column with labels\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",  # One-hot encode labels\n",
        "        subset=\"training\"  # Use training subset\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=df,\n",
        "        directory=image_dir,\n",
        "        x_col=\"path\",\n",
        "        y_col=\"label\",\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"categorical\",\n",
        "        subset=\"validation\"  # Use validation subset\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "# Define paths and preprocess the data\n",
        "# training_df = train_df\n",
        "# train_df['label'] = train_df['label'].astype(str)\n",
        "\n",
        "training_df = df[~df.id.isin(test_df.id)]\n",
        "training_df['label'] = training_df['label'].astype(str)\n",
        "\n",
        "image_dir = '.'  # Path to the image directory\n",
        "train_generator, val_generator = preprocess_data(training_df, image_dir, batch_size=128*2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3aIirig8eh_",
        "outputId": "8c46b545-b657-4376-fbd9-46cda3a63088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-976171bcae79>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  training_df['label'] = training_df['label'].astype(str)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 246282 validated image filenames belonging to 153 classes.\n",
            "Found 2487 validated image filenames belonging to 153 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "# base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# base_model = tf.keras.applications.EfficientNetB2( include_top=False, weights='imagenet')\n",
        "\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = True\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Global Average Pooling\n",
        "x = Dropout(0.5)(x)  # Add Dropout for regularization\n",
        "x = Dense(1024, activation='relu')(x)  # Fully connected layer\n",
        "predictions = Dense(len(train_generator.class_indices), activation='softmax')(x)  # Output layer\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45lTZRc58q3i",
        "outputId": "d98f99e2-11d9-4eee-d6bc-64c987ddddb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in the base model:  175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    # validation_data=val_generator,\n",
        "    epochs=5,  # Number of epochs\n",
        "    # steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    # validation_steps=val_generator.samples // val_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kLRG7Mb8uJ1",
        "outputId": "35b2e778-5284-4a24-8bf3-96ac9218b84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 1s/step - accuracy: 0.1708 - loss: 3.6498\n",
            "Epoch 2/5\n",
            "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1131s\u001b[0m 1s/step - accuracy: 0.4491 - loss: 2.0079\n",
            "Epoch 3/5\n",
            "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1130s\u001b[0m 1s/step - accuracy: 0.5174 - loss: 1.7108\n",
            "Epoch 4/5\n",
            "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 1s/step - accuracy: 0.5513 - loss: 1.5656\n",
            "Epoch 5/5\n",
            "\u001b[1m963/963\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1126s\u001b[0m 1s/step - accuracy: 0.5724 - loss: 1.4758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('polyvore_resnet.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q92Chwu9WOd",
        "outputId": "d3fc1cab-5521-43c4-d0b2-1570e2fea154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqoOigxA9wXN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}